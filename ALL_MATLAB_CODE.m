%% All Matlab Code 
%% List of Contents 
%% 1:Splitting Data into Training and Testing Sets 
%% 2:Feature Selection
%% 3:Defining Response and Explanatory Variables
%% 4:Decision Tree Optimisation
%%   4.1: DT OPT1
%%   4.2: DT OPT1
%%   4.3: Training and Saving DT for Final Time Before Testing
%%   4.4: Final Training Losses Recorded
%% 5:Testing Performance of MdlFinalDT on Unseen Hold Out Set 
%% 6:Random Forest Optimisation
%%   6.1: Notes
%%   6.2: RF OPT1
%%   6.3: RF OPT2
%%   6.4: RF OPT3
%%   6.5: Training and Saving RF for Final Time Before testing
%%   6.6: Final Training Losses Recorded
%% 7:Testing Performance of MdlRFFinal on Unseen Hold Out Set 


%% 1: Splitting Data into Training and Testing Sets
% Firstly, time and high blood pressure columns are dropped as discussed in
% the poster.
% Time is not an available attribute when a new patient is admitted to a
% hospital.
% The measurement boundary needed for someone to be classified with high blood pressure for 
% this dataset was not available online, making it an unreproducible measurement

T = readtable('heart_failure.csv');
data = removevars(T,{'time', 'high_blood_pressure'});

% There is no external validation set available, therefore, the Heart
% Failure dataset is randomly split into a testing and a training set


%rng for reproducibility.
rng("default")
%crossvalind can be used to generate random indices which
%can be used to seperate data randomly into two datasets.
[training,holdout] = crossvalind('Holdout',299,0.3);

FinalTestSet = data(holdout, :);
save('FinalTestSet.mat', "FinalTestSet")
%FinalTestSet not used until final evaluation of model.
%FinalTestSet is saved as 'FinalTestSet.mat'

TrainingSet = data(training, :);

%Defining explanatory and response variables, X and Y.
Y = TrainingSet.DEATH_EVENT;
X = [TrainingSet.anaemia TrainingSet.age TrainingSet.creatinine_phosphokinase TrainingSet.diabetes TrainingSet.ejection_fraction TrainingSet.platelets TrainingSet.serum_creatinine TrainingSet.serum_sodium TrainingSet.sex TrainingSet.smoking];

%% 2: Feature Selection:
rng("default")
% Feature selection was peformed using the p values generated by 
% Binomial logistic regression function.
% p values less than 0.05 indicate statistical significance to Death
% Event in the fitglm("Distribution", "binomial") model.

mdl = fitglm(X, Y, "Distribution", "binomial");
FeatureSelectionTable = evalc('mdl');
% Variables with p values greater than 0.05 were removed, leaving 
% age, ejection_fraction, serum_creatinine to be used.


%% 3: Defining Response and Explanatory Variables to be Used Following Feature Selection
%'TrainingSet' previously defined.
Y = TrainingSet.DEATH_EVENT;
%Xadjusted contains only the selected explanatory variables.
Xadjusted = [TrainingSet.age TrainingSet.ejection_fraction TrainingSet.serum_creatinine];

%% 4: Decision Tree Optimisation
%% 4.1 DT OPT1: Initial Grid Search Optimisation Using Wide Range of Minimum Leaf Size Values and Default Range of Maximum Number of Splits
rng("default")

paramsA = hyperparameters('fitctree',Xadjusted,Y);

%paramsA(1) refers to MinLeafSize
paramsA(1).Range = [1,200];
paramsA(1).Optimize = true;

%paramsA(2) refers to MaxSplitSize
paramsA(2).Optimize = true;

%MinLeafSize range changed to wide range of [1,200].
%MaxNumSplits used default range  of [1,209].
%Gridsearch optimisation used and number of griddivision increased to 20.
%All optimisations performed using k-fold cross validation.
MdlDT1 = fitctree(Xadjusted,Y, 'OptimizeHyperparameters',[paramsA(1) paramsA(2)],'HyperparameterOptimizationOptions',struct('Optimizer', 'gridsearch', 'NumGridDivisions', 20, 'KFold',10));
%%
% View results of DT Optimisation 1
DT_Opt1= sortrows(MdlDT1.HyperparameterOptimizationResults)
DT_Opt1_Vis = scatter3(DT_Opt1.MinLeafSize, DT_Opt1.MaxNumSplits, DT_Opt1.Objective);
title('DT Parameter Optimisation 1: Cross-Validated Loss Vs Max. Number of Splits and Min. Leaf Size')
xlabel('Minimum Leaf Size')
zlabel('Cross-Validated Classification Loss')
ylabel('Maximum Number of Splits')
text(21,4, 0.28, {'Numerous parameter combinations gave the minimum loss' , 'such as 4 for Max. Number of Splits and 21 for Min. Leaf Size' , 'which gave a 0.2381 loss'})
shg

saveas(gcf,'DT_Optimisation_1.png')

%Lowest loss achieved with MinLeafSize = 21 and a range of MaxSplitSize
%from 4 to 209.
%MinLeafSize = 21 gave lowest cross-validated classification loss for several values of MaxSplitSize.
%Therefore MinLeafSize = 21 will be used for all proceeding decision tree
%models. Lowest loss was 0.238.
%% 4.2 DT OPT2: Grid Search Optimisation Over Smaller Range of Values for only MaxSplitSize
rng("default")
paramsB = hyperparameters('fitctree',Xadjusted,Y);

%paramsB(1) refers to MinLeafSize
paramsB(1).Optimize = false;

%paramsB(2) refers to MaxSplitSize
% 1 to 100 range used for MaxplitSize
paramsB(2).Range = [1, 100];
paramsB(2).Optimize = true;

%Using MinLeafSize of 21 following previous optimisation in step 4.1
MdlDT2 = fitctree(Xadjusted,Y, 'MinLeafSize', 21, 'OptimizeHyperparameters',paramsB(2),'HyperparameterOptimizationOptions',struct('Optimizer', 'gridsearch', 'NumGridDivisions', 20, 'KFold',10));
%%
% View results of DT Optimisation 2
DT_Opt2= sortrows(MdlDT2.HyperparameterOptimizationResults)
DT_Opt2Vis = plot(DT_Opt2.MaxNumSplits, DT_Opt2.Objective);
title('DT Parameter Optimisation 2: Cross-Validated Loss Vs Max. Number of Splits')
ylabel('Cross-Validated Classification Loss')
xlabel('Maximum Number of Splits')
shg
saveas(gcf,'DT_Optimisation_2.png')

%Lowest error for MaxSplitSize at numerous values between 4 and 100
%Multiple values give the best error, therefore use smallest size of 4.
%This reduces the risk of overfitting to this dataset, especially important
%as the dataset is small.

%% 4.3: Training and Saving DT for Final Time Before Testing
%Retrain on the whole training set using best parameters.
rng("default")
%tic toc to see final training time
tic
MdlDTFinal = fitctree(Xadjusted,Y, 'MinLeafSize', 21, 'MaxNumSplits', 4);
toc
% SAVING MODEL
save('MdlDTFinal.mat' ,'MdlDTFinal')
%% 4.4: Final Mean Training Loss Recorded
rng("default")

CVMdlDTFinal = crossval(MdlDTFinal);
DTlosses = kfoldLoss(CVMdlDTFinal, 'Mode','individual');
MeanDTlosses = mean(DTlosses)

%% 5: Testing Performance of MdlFinalDT on Unseen Hold Out Set
%Ytest and Xtest defined using 'FinalTrainSet' which was made in Section 1
rng("default")

Ytest = FinalTestSet.DEATH_EVENT;
Xtest = [FinalTestSet.age FinalTestSet.ejection_fraction FinalTestSet.serum_creatinine];

load('MdlDTFinal.mat' ,'MdlDTFinal')
%tic toc to measure execution time.
%time varies 
tic
Ypredictions = predict(MdlDTFinal, Xtest);
toc

%Visualise confusion matrixes
DTResults = confusionmat(Ytest ,Ypredictions);
[cm] = confusionchart(Ytest ,Ypredictions);
shg
saveas(gcf,'ConfusionMatrixDT.png')

DTAccuracy = (53 + 14) / (53+7+15+14)
DTPrecision = 14 / (14+7)
DTRecall = 14/ (15+14)
DTF1_Score = (2*DTRecall*DTPrecision)/(DTRecall+DTPrecision)
DTMCC = (53*14-15*7) / sqrt((14+7)*(14+15)*(53+7)*(53+15))
%% 6. Random Forest Optimisation
%% 6.1 Notes
%"('Reproducible', true)" is used in the template trees to allow results 
% to be reproduced

% 3 variables to be sampled at every split, default is 2. As the number of
% attributes is small, this value is increased to 3.

t1 = templateTree('Reproducible',true, 'NumVariablesToSample', 3);
% Specify the method 'Bag' for all runs, which uses bagging.
%% 6.2 RT OPT1: Optimising MaxNumSplits, MinLeafSize and NumLearningCycles (number of trees)
rng("default")
t1 = templateTree('Reproducible',true, 'NumVariablesToSample', 3);
paramsRTa = hyperparameters('fitcensemble',Xadjusted,Y, t1);
%paramsRTa(2) refers to NumLearningCycles, default range 10 to 500
%paramsRTa(4) refers to MinLeafSize, default range 1 to 105
%paramsRTa(5) refers to MaxSplitSize, default range 1 to 209
paramsRTa(2).Optimize = true;
paramsRTa(4).Optimize = true;
paramsRTa(5).Optimize = true;
%Default ranges are wide enough for initial gridsearch.
%NumGridDivisions is decreased from 20 to 5; RF optimisation is more time
%consuming and 3 parameters are being optimised. Thus, it is not feasible
%to use the same number of griddivisions as in DT optimisation.

rng("default")
MdlRT1 = fitcensemble(Xadjusted, Y,'Method', 'Bag', 'Learners', t1 ,'OptimizeHyperparameters',[paramsRTa(2) paramsRTa(4) paramsRTa(5)], 'HyperparameterOptimizationOptions',struct('Optimizer', 'gridsearch', 'NumGridDivisions', 4))
%Lowest Loss: [Loss = 0.2333, NumLearning cycles = 136, MinLeafSize = 5 and
%MaxNumSplits = 35] 
%% 6.3 RT OPT2: Repeat Gridsearch Over Smaller Range of Values for Each Parameter 
rng('default')
t1 = templateTree('Reproducible',true, 'NumVariablesToSample', 3);
paramsRTb = hyperparameters('fitcensemble',Xadjusted,Y, t1);
%paramsRTb(2) refers to NumLearningCycles, updated range: 10 to 200
%paramsRTb(4) refers to MinLeafSize, updated range: 1 to 50
%paramsRTb(5) refers to MaxSplitSize, updated range: 1 to 75

paramsRTb(2).Range = [1 200];
paramsRTb(2).Optimize = true;
paramsRTb(4).Range = [1 50];
paramsRTb(4).Optimize = true
paramsRTb(5).Range = [1 75];
paramsRTb(5).Optimize = true;

MdlRT2 = fitcensemble(Xadjusted, Y,'Method', 'Bag', 'Learners', t1 ,'OptimizeHyperparameters',[paramsRTb(2) paramsRTb(4) paramsRTb(5)], 'HyperparameterOptimizationOptions',struct('Optimizer', 'gridsearch', 'NumGridDivisions', 5))
%Lowest Losses: [Loss = 0.22857, NumLearning cycles = 53, MinLeafSize = 7 and
%MaxNumSplits = 75] 
%% 6.4 RT OPT3: Final Gridsearch Over Smaller Range of Values for Each Parameter 
rng('default')
t1 = templateTree('Reproducible',true, 'NumVariablesToSample', 3);
paramsRTc = hyperparameters('fitcensemble',Xadjusted,Y, t1);
%paramsRTc(2) refers to NumLearningCycles, updated range: 25 to 75
%paramsRTc(4) refers to MinLeafSize, updated range: 1 to 15
%paramsRTc(5) refers to MaxSplitSize, updated range: 30 to 80

paramsRTc(2).Range = [25 75];
paramsRTc(2).Optimize = true;
paramsRTc(4).Range = [1 15];
paramsRTc(4).Optimize = true;
paramsRTc(5).Range = [30 80];
paramsRTc(5).Optimize = true;

MdlRT3 = fitcensemble(Xadjusted, Y,'Method', 'Bag', 'Learners', t1 ,'OptimizeHyperparameters',[paramsRTc(2) paramsRTc(4) paramsRTc(5)], 'HyperparameterOptimizationOptions',struct('Optimizer', 'gridsearch', 'NumGridDivisions', 5))

%Lowest Loss for this Opt [Loss = 0.2381, NumLearning cycles = 43, MinLeafSize = 4 and
%MaxNumSplits = 80] 

%Lowest Losses still from RT OPT 2: [Loss = 0.22857, NumLearning cycles = 53, MinLeafSize = 7 and
%MaxNumSplits = 75] 

%% 6.5 Training and Saving RF for Final Time Before Testing
%Hyper-parameters from RT OPT2 gave the lowest lost: NumLearning cycles = 53, MinLeafSize = 7 and MaxNumSplits = 75] 
%therefore they are used for the final model.
rng('default')
%tic toc to see final training time
tic
t2 = templateTree('Reproducible',true, 'MaxNumSplits',75 , 'MinLeafSize', 7, 'NumVariablesToSample', 3);
MdlRFFinal = fitcensemble(Xadjusted,Y,'method', 'bag', 'Learner', t2, 'NumLearningCycles', 53);
toc
%SAVING MODEL
save('MdlRFFinal.mat' ,'MdlRFFinal')
%% 6.6 Final Mean Training Loss Recorded
%Check Cross-validation classification losses on whole training set
rng("default")
CVMdlRFFinal = crossval(MdlRFFinal);
RFlosses = kfoldLoss(CVMdlRFFinal, 'Mode','individual');
MeanRFlosses = mean(RFlosses)
%% 7. Testing Performance of MdlRFFinal on Unseen Hold Out Set
rng('default')

%Ytest and Xtest same as previously defined in Section 5.

Ytest = FinalTestSet.DEATH_EVENT;
Xtest = [FinalTestSet.age FinalTestSet.ejection_fraction FinalTestSet.serum_creatinine];

load('MdlRFFinal.mat', 'MdlRFFinal')

%tic toc to record execution time.
tic
labelRF = predict(MdlRFFinal, Xtest);
toc
RFResults = confusionmat(Ytest ,labelRF);
[cm] = confusionchart(Ytest ,labelRF);
shg
saveas(gcf,'ConfusionMatrixRF.png')

RFAccuracy = (49 + 16) / (49+11+16+13)
RFPrecision = 16 / (16+11)
RFRecall = 16/ (16+13)
RFF1_Score = (2*RFRecall*RFPrecision)/(RFRecall+RFPrecision)
RFMCC = (49*16-13*11) / sqrt((16+11)*(16+13)*(49+11)*(49+13))